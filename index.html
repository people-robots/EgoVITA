<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="EgoVITA: Learning to Plan and Verify for Egocentric Video Reasoning">
  <meta name="keywords" content="EgoVITA, Egocentric Video, Reinforcement Learning, GRPO, Video Understanding, MLLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EgoVITA: Learning to Plan and Verify for Egocentric Video Reasoning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>

  <style>
    .placeholder-box {
      padding: 10px;
      margin: 10px 0;
      text-align: center;
      color: #777;
    }
    .placeholder-box img {
        max-width: 100%;
        height: auto;
        display: block;
        margin: 0 auto;
        border-radius: 4px; 
    }

    /* New class to constrain grid images */
    .result-image {
        max-height: 300px;
        width: auto;
        object-fit: contain;
    }

    .animated-gradient-title {
      background-image: linear-gradient(to right, #3273dc, #209cee, #48c774, #ffdd57, #ff6b6b);
      background-size: 300% auto;
      color: transparent;
      background-clip: text;
      -webkit-background-clip: text;
      animation: gradient-animation 5s linear infinite;
      display: inline-block;
      vertical-align: middle;
    }

    @keyframes gradient-animation {
      0% { background-position: 0% center; }
      50% { background-position: 100% center; }
      100% { background-position: 0% center; }
    }

    .title-icon-image {
       height: 1em; 
       width: auto; 
       vertical-align: middle; 
       margin-left: 10px;
       display: inline-block; 
       animation: wobble 2s ease-in-out infinite;
    }

    @keyframes wobble {
      0%, 100% { transform: rotate(0deg) scale(1); }
      25% { transform: rotate(-5deg) scale(1.1); }
      75% { transform: rotate(5deg) scale(0.9); }
    }
    
    .box-gray {
        background-color: #f5f5f5;
        border-radius: 8px;
        padding: 1.5rem;
        height: 100%;
    }
  </style>

</head>

<body>
  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navbarBasicExample">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div id="navbarBasicExample" class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
         <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://people-robots.github.io/VideoSAVi/">
               VideoSAVi
             </a>
             <a class="navbar-item" href="https://people-robots.github.io/VideoPASTA/">
               VideoPASTA
             </a>
             <a class="navbar-item" href="https://people-robots.github.io/AVATAR/">
               AVATAR
             </a>
             </div>
        </div>
       </div>
    </div>
  </nav>

  <section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title is-bold">
             <span class="animated-gradient-title">EgoVITA</span><img src="static/images/logo.jpg" alt="EgoVITA Icon" class="title-icon-image">
          </h1>
           <h2 class="subtitle is-3 publication-subtitle">
             Learning to Plan and Verify for Egocentric Video Reasoning
           </h2>
<div class="is-size-5 publication-authors">

            <span class="author-block"><a href="https://yogkul2000.github.io/">Yogesh Kulkarni</a><sup style="color:#6fbf73;">1</sup>,</span>

            <span class="author-block"><a href="https://www.pooyanfazli.com/">Pooyan Fazli</a><sup style="color:#6fbf73;">1</sup></span>

             </div>
                       <div class="is-size-5 publication-authors">

             <span class="author-block"><sup style="color:#6fbf73;">1</sup>Arizona State University</span><br>

           </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                 <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
               <span class="link-block">
                 <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
               <span class="link-block">
                 <a href="https://github.com/yogkul2000/EgoVITA/" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
             </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="content has-text-centered">
       <div class="placeholder-box">
         <img src="static/images/teaser.png" alt="EgoVITA Framework Overview" style="width: 70%;">
       </div>
       <p>EgoVITA enables safer, structured egocentric reasoning. For first-person queries, it produces grounded, stepwise plans anticipating actions and spatial constraints, whereas baseline models give generic, unsafe, visually ungrounded responses.</p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Reasoning about intentions and actions from a first-person (egocentric) perspective remains a fundamental challenge for multimodal large language models (MLLMs). Unlike third-person (exocentric) videos that capture scenes from an outside observer, egocentric videos reflect the actor's continuously changing viewpoint, introducing partial observability, limited field of view, and self-referenced motion.
          </p>
          <p>
            We introduce EgoVITA, a reinforcement learning framework that enables MLLMs to reason through structured planning and verification. Built on Group Relative Policy Optimization (GRPO), EgoVITA alternates between two stages: (1) an egocentric planning phase, where the model reasons from a first-person viewpoint to predict a step-by-step plan of future actions, and (2) an exocentric verification phase, where it switches to a third-person perspective to check the visual and logical consistency of that plan.
          </p>
           <p>
            Through GRPO, the model learns to make plans that are causally predictive of upcoming visual observations, leading to more coherent and visually grounded reasoning. EgoVITA achieves significant gains on egocentric reasoning tasks, outperforming the baseline Qwen2.5-VL-7B by +7.7 on EgoBlind and +4.4 on EgoOrient, while maintaining strong generalization on exocentric video tasks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3 has-text-centered">Method</h2>
              <div class="content has-text-justified">
                <p>
                  EgoVITA is a reinforcement learning framework built on Group Relative Policy Optimization (GRPO) that addresses egocentric video reasoning through structured planning and verification. The framework separates reasoning into two complementary components that operate from different viewpoints.
                </p>
               
                <h3 class="title is-4">Training Pipeline</h3>
                <div class="placeholder-box">
                  <img src="static/images/pipeline.jpg" alt="EgoVITA Training Pipeline" style="max-width: 50%;">
                </div>
                <p>
                    <b>Stage I: Supervised Fine-Tuning (SFT):</b> The policy model is initialized to learn the structured output format with three components: egocentric planning, exocentric verification, and final answer generation. This stage establishes a stable base policy capable of generating structured outputs.
                </p>
                <p>
                    <b>Stage II: GRPO Optimization:</b> The model generates multiple reasoning trajectories for each video-query pair and scores them using a composite reward function. The policy is then refined based on relative performance within each group, enabling exploration of diverse reasoning paths while maintaining stability.
                </p>

                <h3 class="title is-4">Anticipatory Cross-Modal Grounding (ACMG)</h3>
                
                <div class="columns is-centered is-vcentered">
                    <div class="column">
                        <div class="placeholder-box">
                          <img src="static/images/acmg.jpg" alt="ACMG Mechanism">
                        </div>
                    </div>
                    <div class="column">
                        <div class="placeholder-box">
                          <img src="static/images/acmg_heatmap.jpg" alt="Temporal Grounding Heatmap">
                        </div>
                    </div>
                </div>
                
                <p>
                    ACMG is a novel dense reward mechanism that ensures generated plans are temporally predictive and visually grounded. Each plan clause is projected into visual space via a trainable Anticipation Head and compared to future frames using cosine similarity. The reward measures how well the predicted visual embedding matches any of the next N=16 frames, encouraging the model to anticipate what will happen next rather than just describing the current scene.
                </p>
                <p>
                    The temporal grounding heatmap (right) shows how different plan clauses align with future frames. Earlier clauses ground to near-future frames while later clauses align with more distant events, demonstrating that the model learns temporally structured anticipation.
                </p>

                <h3 class="title is-4">Composite Reward Function & Regularization</h3>
                <p>
                    EgoVITA uses a weighted combination of four reward components: (1) Format Reward, (2) Answer Reward, (3) ACMG Reward, and (4) Confidence Reward. To prevent catastrophic forgetting, we periodically interleave GRPO updates with lightweight exocentric regularization on held-out VideoQA data.
                </p>

             </div>
            </div>
          </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Results</h2>

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4 has-text-centered">Main Results Across Foundation Models</h3>
        <div class="content has-text-justified">
           <p>
               EgoVITA consistently improves egocentric video understanding across multiple foundation models. On Qwen2.5-VL-7B, it achieves substantial gains of +7.7 on EgoBlind, +3.7 on EgoThink, and +4.4 on EgoOrient. Importantly, EgoVITA not only improves egocentric reasoning but also maintains or enhances performance on exocentric benchmarks.
           </p>
           <div class="placeholder-box">
               <img src="static/images/main_table.png" alt="Main Results Table">
           </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered is-multiline">
        <div class="column is-5">
            <h3 class="title is-5 has-text-centered">Comparison with SOTA</h3>
            <div class="content has-text-justified is-size-6">
               <p>
                   EgoVITA outperforms recent egocentric reasoning models including EgoThinker and EgoVLM. Despite using far fewer training samples (47k vs. 5M for EgoThinker), EgoVITA achieves higher accuracy, demonstrating the effectiveness of dense multimodal rewards.
               </p>
               <div class="placeholder-box">
                   <img src="static/images/sota_compare.png" alt="State-of-the-Art Comparison" class="result-image">
               </div>
            </div>
        </div>
        <div class="column is-5">
            <h3 class="title is-5 has-text-centered">Embedding Space Analysis</h3>
            <div class="content has-text-justified is-size-6">
               <p>
                   t-SNE visualization of the ACMG embedding space reveals semantically structured clusters. Strong alignment between text clauses, predicted visual embeddings, and actual matched frames confirms that EgoVITA learns meaningful task semantics.
               </p>
               <div class="placeholder-box">
                   <img src="static/images/embedding.png" alt="ACMG Embedding Space Visualization" class="result-image">
               </div>
            </div>
        </div>
    </div>

    <br>
    
    <div class="columns is-centered">
        <div class="column is-four-fifths">
            <h3 class="title is-4 has-text-centered">Ablation & Exocentric Analysis</h3>
            <p class="has-text-centered is-size-6 mb-4">Comprehensive studies validate the importance of each component and the stability of the framework.</p>
            
            <div class="columns is-multiline is-centered">
                <div class="column is-6">
                    <div class="box-gray">
                        <h4 class="title is-6 has-text-centered">Reward Components</h4>
                        <div class="placeholder-box">
                            <img src="static/images/reward.png" alt="Reward Component Ablation" class="result-image">
                        </div>
                        <p class="is-size-7">The ACMG and Confidence rewards are complementary, with the full system achieving greater improvements than either alone.</p>
                    </div>
                </div>

                <div class="column is-6">
                    <div class="box-gray">
                        <h4 class="title is-6 has-text-centered">Reasoning Structure</h4>
                        <div class="placeholder-box">
                            <img src="static/images/structure.png" alt="Structure Ablation" class="result-image">
                        </div>
                        <p class="is-size-7">Removing either the egocentric planning or exocentric verification stage degrades performance on their respective task types.</p>
                    </div>
                </div>

                <div class="column is-6">
                    <div class="box-gray">
                        <h4 class="title is-6 has-text-centered">Anticipation vs Present</h4>
                        <div class="placeholder-box">
                            <img src="static/images/acmg_vs_present.png" alt="Anticipatory vs Present Grounding" class="result-image">
                        </div>
                        <p class="is-size-7">Comparison with present-frame grounding confirms that anticipatory grounding substantially outperforms describing only the current scene.</p>
                    </div>
                </div>

                <div class="column is-6">
                    <div class="box-gray">
                        <h4 class="title is-6 has-text-centered">Exocentric Regularization</h4>
                        <div class="placeholder-box">
                            <img src="static/images/exo.png" alt="Exocentric Regularization Impact" class="result-image">
                        </div>
                        <p class="is-size-7">Regularization (solid red line) reverses the catastrophic forgetting observed in standard SFT (dotted red line), recovering performance.</p>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4 has-text-centered">Qualitative Examples</h3>
        <div class="content has-text-justified">
           <p>
               Qualitative comparisons demonstrate EgoVITA's superior reasoning capabilities. For a blind person crossing an intersection, EgoVITA generates sequential, safety-critical actions grounded in specific visual frames.
           </p>
           <div class="placeholder-box">
               <img src="static/images/qual.png" alt="Qualitative Examples">
           </div>
        </div>
      </div>
    </div>

    </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <pre><code>@article{kulkarniegovita2025,
  title={EgoVITA: Learning to Plan and Verify for Egocentric Video Reasoning},
  author={Yogesh Kulkarni and Pooyan Fazli},
  journal={arXiv preprint arXiv:{xxxxx.xxxxx}},
  year={2025}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
     </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            Website template adapted from <a href="https://nerfies.github.io/">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


<script>
document.addEventListener('DOMContentLoaded', () => {
  const $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0);
  if ($navbarBurgers.length > 0) {
    $navbarBurgers.forEach( el => {
      el.addEventListener('click', () => {
        const target = el.dataset.target;
        const $target = document.getElementById(target);
        el.classList.toggle('is-active');
        if ($target) {
            $target.classList.toggle('is-active');
        }
      });
    });
  }
});
</script>

</body>
</html>