<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="EgoVITA: Learning to Plan and Verify for Egocentric Video Reasoning">
  <meta name="keywords" content="EgoVITA, Egocentric Video, Reinforcement Learning, GRPO, Video Understanding, MLLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EgoVITA: Learning to Plan and Verify for Egocentric Video Reasoning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>

  <style>
    .placeholder-box {
      padding: 10px;
      margin: 10px 0;
      text-align: center;
      color: #777;
    }
    .placeholder-box img {
        max-width: 100%;
        height: auto;
        display: block;
        margin: 0 auto;
    }

    .animated-gradient-title {
      background-image: linear-gradient(to right, #3273dc, #209cee, #48c774, #ffdd57, #ff6b6b);
      background-size: 300% auto;
      color: transparent;
      background-clip: text;
      -webkit-background-clip: text;
      animation: gradient-animation 5s linear infinite;
      display: inline-block;
      vertical-align: middle;
    }

    @keyframes gradient-animation {
      0% { background-position: 0% center; }
      50% { background-position: 100% center; }
      100% { background-position: 0% center; }
    }

    .title-icon-image {
       height: 1em; 
       width: auto; 
       vertical-align: middle; 
       margin-left: 10px;
       display: inline-block; 
       animation: wobble 2s ease-in-out infinite;
    }

    @keyframes wobble {
      0%, 100% { transform: rotate(0deg) scale(1); }
      25% { transform: rotate(-5deg) scale(1.1); }
      75% { transform: rotate(5deg) scale(0.9); }
    }

  </style>

</head>

<body>
  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navbarBasicExample">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div id="navbarBasicExample" class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
         <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://people-robots.github.io/VideoSAVi/">
               VideoSAVi
             </a>
             <a class="navbar-item" href="https://people-robots.github.io/VideoPASTA/">
               VideoPASTA
             </a>
             <a class="navbar-item" href="https://people-robots.github.io/AVATAR/">
               AVATAR
             </a>
             </div>
        </div>
       </div>
    </div>
  </nav>

  <section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title is-bold">
             <span class="animated-gradient-title">EgoVITA</span><img src="static/images/logo.jpg" alt="EgoVITA Icon" class="title-icon-image">
          </h1>
           <h2 class="subtitle is-3 publication-subtitle">
             Learning to Plan and Verify for Egocentric Video Reasoning
           </h2>
           <div class="is-size-5 publication-authors">
            <span class="author-block">Anonymous CVPR Submission</span>
             </div>

          <div class="is-size-5 publication-authors">
             <span class="author-block">Paper ID 9371</span><br>
           </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                 <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
               <span class="link-block">
                 <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
               <span class="link-block">
                 <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>
             </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="content has-text-centered">
       <div class="placeholder-box">
         <img src="static/images/figure1_teaser.png" alt="EgoVITA Framework Overview" style="width: 70%;">
       </div>
       <p>EgoVITA enables safer, structured egocentric reasoning. For first-person queries, it produces grounded, stepwise plans anticipating actions and spatial constraints, whereas baseline models give generic, unsafe, visually ungrounded responses.</p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Reasoning about intentions and actions from a first-person (egocentric) perspective remains a fundamental challenge for multimodal large language models (MLLMs). Unlike third-person (exocentric) videos that capture scenes from an outside observer, egocentric videos reflect the actor's continuously changing viewpoint, introducing partial observability, limited field of view, and self-referenced motion.
          </p>
          <p>
            We introduce EgoVITA, a reinforcement learning framework that enables MLLMs to reason through structured planning and verification. Built on Group Relative Policy Optimization (GRPO), EgoVITA alternates between two stages: (1) an egocentric planning phase, where the model reasons from a first-person viewpoint to predict a step-by-step plan of future actions, and (2) an exocentric verification phase, where it switches to a third-person perspective to check the visual and logical consistency of that plan.
          </p>
           <p>
            Through GRPO, the model learns to make plans that are causally predictive of upcoming visual observations, leading to more coherent and visually grounded reasoning. EgoVITA achieves significant gains on egocentric reasoning tasks, outperforming the baseline Qwen2.5-VL-7B by +7.7 on EgoBlind and +4.4 on EgoOrient, while maintaining strong generalization on exocentric video tasks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3 has-text-centered">Method</h2>
              <div class="content has-text-justified">
                <p>
                  EgoVITA is a reinforcement learning framework built on Group Relative Policy Optimization (GRPO) that addresses egocentric video reasoning through structured planning and verification. The framework separates reasoning into two complementary components that operate from different viewpoints.
                </p>
               
                <h3 class="title is-4">Training Pipeline</h3>
                <div class="placeholder-box">
                  <img src="static/images/figure2_pipeline.png" alt="EgoVITA Training Pipeline">
                </div>
                <p>
                    <b>Stage I: Supervised Fine-Tuning (SFT):</b> The policy model is initialized to learn the structured output format with three components: egocentric planning, exocentric verification, and final answer generation. This stage establishes a stable base policy capable of generating structured outputs.
                </p>
                <p>
                    <b>Stage II: GRPO Optimization:</b> The model generates multiple reasoning trajectories for each video-query pair and scores them using a composite reward function. The policy is then refined based on relative performance within each group, enabling exploration of diverse reasoning paths while maintaining stability.
                </p>

                <h3 class="title is-4">Anticipatory Cross-Modal Grounding (ACMG)</h3>
                <div class="placeholder-box">
                  <img src="static/images/figure3_acmg.png" alt="ACMG Mechanism" width="70%">
                </div>
                <p>
                    ACMG is a novel dense reward mechanism that ensures generated plans are temporally predictive and visually grounded. Each plan clause is projected into visual space via a trainable Anticipation Head and compared to future frames using cosine similarity. The reward measures how well the predicted visual embedding matches any of the next N=16 frames, encouraging the model to anticipate what will happen next rather than just describing the current scene.
                </p>

                <div class="placeholder-box">
                  <img src="static/images/figure4_temporal_grounding.png" alt="Temporal Grounding Heatmap" width="70%">
                </div>
                <p>
                    The temporal grounding heatmap shows how different plan clauses align with future frames. Earlier clauses ground to near-future frames while later clauses align with more distant events, demonstrating that the model learns temporally structured anticipation.
                </p>

                <h3 class="title is-4">Composite Reward Function</h3>
                <p>
                    EgoVITA uses a weighted combination of four reward components: (1) Format Reward checks for correct tag usage and output structure, (2) Answer Reward measures factual alignment with ground truth, (3) ACMG Reward promotes temporal and visual grounding of predicted plans, and (4) Confidence Reward encourages coherent and consistent reasoning through log-probability alignment.
                </p>

                <h3 class="title is-4">Exocentric Regularization</h3>
                <p>
                    To prevent catastrophic forgetting of general visual reasoning capabilities, EgoVITA periodically interleaves GRPO updates with lightweight exocentric regularization. After every 200 RL iterations, the model is evaluated on held-out exocentric VideoQA data, preserving general understanding while enabling specialization on egocentric tasks.
                </p>

             </div>
            </div>
          </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Results</h2>

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4 has-text-centered">Main Results Across Foundation Models</h3>
        <div class="content has-text-justified">
           <p>
               EgoVITA consistently improves egocentric video understanding across multiple foundation models. On Qwen2.5-VL-7B, it achieves substantial gains of +7.7 on EgoBlind, +3.7 on EgoThink, and +4.4 on EgoOrient. The framework also demonstrates strong performance on InternVL-3.5 and Qwen3-VL, with improvements that are statistically significant at Î± = 0.05.
           </p>
           <div class="placeholder-box">
               <img src="static/images/table1_main_results.png" alt="Main Results Table">
           </div>
           <p>
               Importantly, EgoVITA not only improves egocentric reasoning but also maintains or enhances performance on exocentric benchmarks, achieving gains on MVBench, Video-MME, and LVBench. This demonstrates effective view-invariant learning without catastrophic forgetting.
           </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4 has-text-centered">Comparison with State-of-the-Art</h3>
        <div class="content has-text-justified">
           <p>
               EgoVITA outperforms recent egocentric reasoning models including EgoThinker and EgoVLM on multiple benchmarks. Despite using far fewer training samples (47k vs. 5M for EgoThinker), EgoVITA achieves higher accuracy on EgoSchema (75.8 vs. 67.6) and EgoPlan (35.0 vs. 33.1), demonstrating the effectiveness of dense multimodal rewards for guided planning and verification.
           </p>
           <div class="placeholder-box">
               <img src="static/images/table2_sota_comparison.png" alt="State-of-the-Art Comparison">
           </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4 has-text-centered">Ablation Studies</h3>
        <div class="content has-text-justified">
           <p>
               Comprehensive ablation studies validate the importance of each component. The ACMG and Confidence rewards are complementary, with the full system achieving greater improvements than either reward alone. The disentangled reasoning structure is essential, as removing either the egocentric planning or exocentric verification stage degrades performance on their respective task types.
           </p>
           <div class="placeholder-box">
               <img src="static/images/table3_ablation_rewards.png" alt="Reward Component Ablation">
           </div>
           <div class="placeholder-box">
               <img src="static/images/table4_ablation_structure.png" alt="Structure Ablation">
           </div>
           <p>
               Analysis of anticipation window size (N) shows that N=16 provides optimal balance between capturing delayed actions and avoiding spurious correlations. Comparison with present-frame grounding confirms that anticipatory grounding substantially outperforms describing only the current scene.
           </p>
           <div class="placeholder-box">
               <img src="static/images/table5_anticipation_vs_present.png" alt="Anticipatory vs Present Grounding">
           </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4 has-text-centered">Embedding Space Analysis</h3>
        <div class="content has-text-justified">
           <p>
               t-SNE visualization of the ACMG embedding space reveals semantically structured clusters corresponding to procedural steps (Move, Locate, Action, Verify, Complete). Strong alignment between text clauses, predicted visual embeddings, and actual matched frames confirms that EgoVITA learns meaningful, grounded task semantics.
           </p>
           <div class="placeholder-box">
               <img src="static/images/figure5_embedding_space.png" alt="ACMG Embedding Space Visualization" width="60%">
           </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4 has-text-centered">Impact of Exocentric Regularization</h3>
        <div class="content has-text-justified">
           <p>
               Training on egocentric data via SFT causes an initial performance drop on exocentric benchmarks due to overfitting. Without regularization, this decline continues throughout training. EgoVITA's exocentric regularization successfully reverses this trend, recovering and ultimately improving upon the original base model's performance across MVBench, Video-MME, LVBench, and TOMATO.
           </p>
           <div class="placeholder-box">
               <img src="static/images/figure1_supp_regularization.png" alt="Exocentric Regularization Impact">
           </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4 has-text-centered">Qualitative Examples</h3>
        <div class="content has-text-justified">
           <p>
               Qualitative comparisons demonstrate EgoVITA's superior reasoning capabilities. For a blind person crossing an intersection, EgoVITA generates sequential, safety-critical actions grounded in specific visual frames, while the baseline provides generic, unsafe recommendations. For procedural tasks like grapevine pruning, EgoVITA specifies systematic, step-by-step approaches with exocentric verification confirming visual consistency.
           </p>
           <div class="placeholder-box">
               <img src="static/images/figure6_qualitative.png" alt="Qualitative Examples">
           </div>
           <div class="placeholder-box">
               <img src="static/images/figure4_supp_qualitative2.png" alt="Additional Qualitative Examples">
           </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4 has-text-centered">Detailed ACMG Analysis</h3>
        <div class="content has-text-justified">
           <p>
               Analysis of 500 egocentric plan samples reveals that ACMG provides position-sensitive grounding signals. Middle clauses (target localization and action execution) receive significantly higher rewards and show stronger correlations with final answer correctness compared to navigation or completion steps, demonstrating adaptive selectivity for task-critical visual grounding.
           </p>
           <div class="placeholder-box">
               <img src="static/images/figure2_supp_acmg_single_clause.png" alt="ACMG Single Clause Analysis">
           </div>
           <div class="placeholder-box">
               <img src="static/images/figure3_supp_acmg_position.png" alt="ACMG Reward by Position" width="60%">
           </div>
        </div>
      </div>
    </div>

    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Additional Ablations</h2>
    
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <h3 class="title is-4 has-text-centered">Pooling Strategy for ACMG</h3>
          <p>
            Max pooling is crucial for ACMG's effectiveness. Mean pooling dilutes the reward signal by averaging across all N=16 frames, punishing correct but sparse temporal alignments. Max pooling finds peak evidence that predicted events occur within the window, providing stronger learning signals while remaining robust to temporal shifts.
          </p>
          <div class="placeholder-box">
            <img src="static/images/table1_supp_pooling.png" alt="Pooling Strategy Ablation">
          </div>

          <h3 class="title is-4 has-text-centered">Teacher-Guided Warm-up Duration</h3>
          <p>
            The 200-step teacher-guided warm-up phase for the confidence reward provides useful initialization, though the self-ranking mechanism proves robust enough to achieve substantial gains even without explicit teacher supervision. The warm-up helps the model learn stable verification patterns before transitioning to self-improvement.
          </p>
          <div class="placeholder-box">
            <img src="static/images/table2_supp_warmup.png" alt="Teacher Warm-up Ablation">
          </div>

          <h3 class="title is-4 has-text-centered">Anticipation Window Sensitivity</h3>
          <p>
            Performance is sensitive to the anticipation window size N. Small windows (N=4 or N=8) are too myopic to capture delayed actions, while large windows (N=32) introduce noisy spurious correlations. N=16 provides optimal balance between flexibility for temporal shifts and precision for causal predictions.
          </p>
          <div class="placeholder-box">
            <img src="static/images/table3_supp_window.png" alt="Anticipation Window Ablation">
          </div>

          <h3 class="title is-4 has-text-centered">Vision Component Training</h3>
          <p>
            Joint adaptation of both vision encoder and projector is critical for optimal performance. Training with frozen vision features yields minimal gains (+2.4), while adapting only the projector or encoder shows intermediate improvements. The full system with end-to-end visual adaptation achieves the strongest results (+7.7 on EgoBlind).
          </p>
          <div class="placeholder-box">
            <img src="static/images/table6_supp_vision_components.png" alt="Vision Component Training Ablation">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <pre><code>@inproceedings{egovita2026,
  title={EgoVITA: Learning to Plan and Verify for Egocentric Video Reasoning},
  author={Anonymous},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2026}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
     </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            Website template adapted from <a href="https://nerfies.github.io/">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
document.addEventListener('DOMContentLoaded', () => {
  const $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0);
  if ($navbarBurgers.length > 0) {
    $navbarBurgers.forEach( el => {
      el.addEventListener('click', () => {
        const target = el.dataset.target;
        const $target = document.getElementById(target);
        el.classList.toggle('is-active');
        if ($target) {
            $target.classList.toggle('is-active');
        }
      });
    });
  }
});
</script>

</body>
</html>